I"a<p>Write your post content here in normal <code class="highlighter-rouge">markdown</code>. An example post is shown below for reference.</p>

<h3 id="introduction">Introduction</h3>
<p>Regularization is an important step during the training of neural networks. It helps to generalize the model by reducing the possibility of overfitting the training data. There are several types of regularization techniques, with L2 , L1, elastic-net (linear combination of L2 and L1 regularization), dropout and drop-connect being the major ones. While L2, L1 and elastic-net regularization techniques work by constraining the trainable parameters (or <em>weights</em>) from attaining large values (so that no drastic changes in output are observed for slight changes in the input; or in other words, they prefer <em>diffused</em> weights rather than <em>peaked</em> ones), <strong>dropout</strong> and drop-connect work by averaging the task over a dynamically and randomly generated large <em>ensemble</em> of networks. These networks are obtained by randomly disconnecting neurons (<em>dropout</em>) or weights (<em>drop-connect</em>) from the original network so as to obtain a subnetwork on which the training process is carried out (although for <script type="math/tex">\approx1</script> epoch for each, since the chance of the same subnetwork being generated again is very rare).</p>
:ET